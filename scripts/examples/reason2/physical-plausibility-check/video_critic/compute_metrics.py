#!/usr/bin/env python3
# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Script to compute accuracy and correlation metrics between ground truth and predicted scores
from JSON result files generated by inference_videophy2.py
"""

import argparse
import glob
import json
import os
import sys
from typing import List, Tuple

import numpy as np

# Files to ignore when loading scores
IGNORE_FILES = ["summary.json"]


def load_scores_from_json_files(output_dir: str) -> Tuple[List[float], List[float]]:
    """
    Load ground truth and predicted scores from all JSON files in the output directory.

    Args:
        output_dir: Directory containing JSON result files

    Returns:
        Tuple of (ground_truth_scores, predicted_scores)
    """
    json_pattern = os.path.join(output_dir, "*.json")
    json_files = sorted(glob.glob(json_pattern))

    # Exclude ignored files
    json_files = [f for f in json_files if os.path.basename(f) not in IGNORE_FILES]

    if not json_files:
        print(f"‚ùå No JSON files found in directory: {output_dir}")
        return [], []

    print(f"üìÅ Found {len(json_files)} JSON files in {output_dir}")

    ground_truth_scores = []
    predicted_scores = []
    failed_files = []

    for json_file in json_files:
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            # Extract scores
            gt_score = data.get("ground_truth")
            pred_score = data.get("pred_score")

            if gt_score is None or pred_score is None:
                print(f"‚ö†Ô∏è  Missing scores in {os.path.basename(json_file)}")
                failed_files.append(json_file)
                continue

            ground_truth_scores.append(float(gt_score))
            predicted_scores.append(float(pred_score))

        except Exception as e:
            print(f"‚ùå Error reading {os.path.basename(json_file)}: {str(e)}")
            failed_files.append(json_file)

    print(f"‚úÖ Successfully loaded scores from {len(ground_truth_scores)} files")
    if failed_files:
        print(f"‚ö†Ô∏è  Failed to load {len(failed_files)} files")

    return ground_truth_scores, predicted_scores


def compute_metrics(gt_scores: List[float], pred_scores: List[float]) -> dict:
    """
    Compute accuracy and correlation metrics between ground truth and predicted scores.

    Args:
        gt_scores: List of ground truth scores
        pred_scores: List of predicted scores

    Returns:
        Dictionary containing accuracy and correlation metrics
    """
    if len(gt_scores) != len(pred_scores):
        raise ValueError("Ground truth and predicted scores must have the same length")

    if len(gt_scores) == 0:
        raise ValueError("No scores to compute metrics")

    gt_array = np.array(gt_scores)
    pred_array = np.array(pred_scores)

    # Accuracy: percentage of exact matches when cast to integers
    gt_int = gt_array.astype(int)
    pred_int = pred_array.astype(int)
    exact_matches = gt_int == pred_int
    accuracy = np.mean(exact_matches)

    # Pearson correlation coefficient
    correlation_matrix = np.corrcoef(gt_array, pred_array)
    pearson_correlation = (
        correlation_matrix[0, 1] if correlation_matrix.shape == (2, 2) else 0.0
    )

    return {
        "accuracy": accuracy,
        "pearson_correlation": pearson_correlation,
        "num_samples": len(gt_array),
    }


def main():
    parser = argparse.ArgumentParser(
        description="Compute accuracy and correlation between ground truth and predicted scores from JSON files"
    )
    parser.add_argument(
        "output_dir", type=str, help="Directory containing JSON result files"
    )

    args = parser.parse_args()

    # Validate output directory
    if not os.path.exists(args.output_dir):
        print(f"‚ùå Output directory does not exist: {args.output_dir}")
        sys.exit(1)

    if not os.path.isdir(args.output_dir):
        print(f"‚ùå Path is not a directory: {args.output_dir}")
        sys.exit(1)

    print(f"üîç Reading JSON files from: {args.output_dir}")

    # Load scores from JSON files
    gt_scores, pred_scores = load_scores_from_json_files(args.output_dir)

    if not gt_scores:
        print("‚ùå No valid scores found for metrics computation.")
        sys.exit(1)

    # Compute metrics
    try:
        metrics = compute_metrics(gt_scores, pred_scores)

        # Print metrics
        print(f"\nüéØ Metrics Results:")
        print(f"   Accuracy: {metrics['accuracy']:.6f}")
        print(f"   Pearson Correlation: {metrics['pearson_correlation']:.6f}")
        print(f"   Number of samples: {metrics['num_samples']}")

        # Save summary to JSON file
        summary_path = os.path.join(args.output_dir, "summary.json")
        with open(summary_path, "w") as f:
            json.dump(metrics, f, indent=2)
        print(f"   Summary saved to: {summary_path}")

    except Exception as e:
        print(f"‚ùå Error computing metrics: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
