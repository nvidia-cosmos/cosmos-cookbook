{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b10dcc",
   "metadata": {},
   "source": [
    "# Worker Safety in a Classical Warehouse\n",
    "### Using Cosmos-Reason2   \n",
    "### [Main recipe](../../../docs/recipes/inference/reason2/worker_safety/inference.md)\n",
    "\n",
    "![Worker safety overview](../../../docs/recipes/inference/reason2/worker_safety/assets/assets_1_worker_safety.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a29a62",
   "metadata": {},
   "source": [
    "This notebook implements a complete Zero-Shot Video Reasoning Pipeline for industrial safety compliance. It demonstrates how to use NVIDIA Cosmos Reason 2, a multimodal Video Language Model (VLM), to act as an automated safety inspector in challenging \"brownfield\" environments.\n",
    "\n",
    "Unlike modern, pristine factories, \"classical\" warehouses often feature faded floor markings, irregular lighting, and worn infrastructure. Standard computer vision models often struggle here, confusing environmental noise with hazards. In this workflow, we solve this by using Context-Aware Prompt Engineering to force the model to ignore the background and focus strictly on specific visual ground truths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50bd992",
   "metadata": {},
   "source": [
    "### 1. Install dependencies \n",
    "\n",
    "Follow the instructions in the [setup](https://github.com/NVIDIA/Cosmos-Cookbook/blob/main/docs/recipes/inference/reason2/worker_safety/setup.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e9e40",
   "metadata": {},
   "source": [
    "### 2. Load dataset and copy the first video to assets/sample.mp4 - Testing the installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d23ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load dataset and copy the first video to assets/sample.mp4\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "ROOT = pathlib.Path.cwd()\n",
    "ASSETS = ROOT / \"assets\"\n",
    "ASSETS.mkdir(exist_ok=True)\n",
    "\n",
    "# NOTE: omit overwrite=True so existing annotations are preserved\n",
    "dataset = fouh.load_from_hub(\"pjramg/Safe_Unsafe_Test\", persistent=True)\n",
    "#dataset = fo.load_dataset(\"pjramg/Safe_Unsafe_Test\") # \n",
    "\n",
    "sample = dataset.first()\n",
    "\n",
    "if sample is None:\n",
    "    raise RuntimeError(\"Dataset is empty\")\n",
    "if sample.media_type != \"video\":\n",
    "    raise RuntimeError(f\"First sample is not a video (media_type={sample.media_type}). Use --images instead.\")\n",
    "\n",
    "dst = ASSETS / \"sample.mp4\"\n",
    "shutil.copy2(sample.filepath, dst)\n",
    "print(\"Reference video copied to:\", dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82250845",
   "metadata": {},
   "source": [
    "### 3. CUDA / environment sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d331721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device index: 0\n",
      "Device name: NVIDIA RTX PRO 5000 Blackwell Generation Laptop GPU\n",
      "CUDA_VISIBLE_DEVICES: None\n"
     ]
    }
   ],
   "source": [
    "# 3. CUDA / environment sanity check\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device index:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87537dbe",
   "metadata": {},
   "source": [
    "### 4. Run the default inference_sample.py file \n",
    "\n",
    "Be sure you are addressing the right sample.mp4 file, check the inference_sample.py file in the Cosmos Reason 2 repository. Make modification to the prompt if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a3ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "```json\n",
      "{\n",
      "  \"prediction_class_id\": 3,\n",
      "  \"prediction_label\": \"Carrying Overload with Forklift\",\n",
      "  \"video_description\": \"A forklift is seen carrying three or more blocks, which is considered unsafe.\",\n",
      "  \"hazard_detection\": {\n",
      "    \"is_hazardous\": true,\n",
      "    \"temporal_segment\": \"0.1 - 5.8\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "--------------------\n",
      "[W122 12:18:07.806639126 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/inference_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b45872",
   "metadata": {},
   "source": [
    "### 5. Initialize the model and create the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70521c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env -S uv run --script\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import transformers\n",
    "import fiftyone as fo\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "def load_model():\n",
    "    model_name = \"nvidia/Cosmos-Reason2-2B\"\n",
    "    model = transformers.Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "        model_name, dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\"\n",
    "    )\n",
    "    processor = transformers.Qwen3VLProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    # Pixel token limits\n",
    "    PIXELS_PER_TOKEN = 32**2\n",
    "    min_vision_tokens, max_vision_tokens = 256, 8192\n",
    "    processor.image_processor.size = processor.video_processor.size = {\n",
    "        \"shortest_edge\": min_vision_tokens * PIXELS_PER_TOKEN,\n",
    "        \"longest_edge\": max_vision_tokens * PIXELS_PER_TOKEN,\n",
    "    }\n",
    "    return model, processor\n",
    "\n",
    "# --- PROMPTS ---\n",
    "SYSTEM_INSTRUCTIONS =\"\"\"\n",
    "    You are an expert Industrial Safety Inspector monitoring a manufacturing facility.\n",
    "    Your goal is to classify the video into EXACTLY ONE of the 8 classes defined below.\n",
    "    \n",
    "    CRITICAL NEGATIVE CONSTRAINTS (What to IGNORE):\n",
    "    1. IGNORE SITTING WORKERS:\n",
    "       - If a person is SITTING at a machine board working, this is NOT an intervention class. Ignore them.\n",
    "       - If a person is SITTING driving a forklift, the driver is NOT the class. Focus only on the LOAD carried.\n",
    "    2. IGNORE BACKGROUND:\n",
    "       - The facility is old. Do not report hazards based on faded floor markings or unpainted areas.\n",
    "    3. SINGLE OUTPUT:\n",
    "       - Even if multiple things happen, choose the MOST PROMINENT behavior.\n",
    "       - Prioritize UNSAFE behaviors over SAFE behaviors if both are present.\n",
    "\"\"\"  \n",
    "\n",
    "USER_PROMPT_CONTENT = \"\"\"\n",
    "    Analyze the video and output a JSON object. You MUST select the class ID and Label EXACTLY from the table below.\n",
    "    \n",
    "    STRICT CLASSIFICATION TABLE (Use these exact IDs and Labels):\n",
    "    \n",
    "    | ID | Label | Definition (Ground Truth) | Hazard Status |\n",
    "    | :--- | :--- | :--- | :--- |\n",
    "    | 0 | Safe Walkway Violation | Worker walks OUTSIDE the designated Green Path. | TRUE (Unsafe) |\n",
    "    | 4 | Safe Walkway | Worker walks INSIDE the designated Green Path. | FALSE (Safe) |\n",
    "    | 1 | Unauthorized Intervention | Worker interacts with machine board WITHOUT a green vest. | TRUE (Unsafe) |\n",
    "    | 5 | Authorized Intervention | Worker interacts with machine board WITH a green vest. | FALSE (Safe) |\n",
    "    | 2 | Opened Panel Cover | Machine panel cover is left OPEN after intervention. | TRUE (Unsafe) |\n",
    "    | 6 | Closed Panel Cover | Machine panel cover is CLOSED after intervention. | FALSE (Safe) |\n",
    "    | 3 | Carrying Overload with Forklift | Forklift carries 3 OR MORE blocks. | TRUE (Unsafe) |\n",
    "    | 7 | Safe Carrying | Forklift carries 2 OR FEWER blocks. | FALSE (Safe) |\n",
    "\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. Identify the behavior in the video.\n",
    "    2. Match it to one row in the table above.\n",
    "    3. Output the exact \"ID\" and \"Label\" from that row. Do not invent new labels like \"safe and compliant\".\n",
    "    \n",
    "    OUTPUT FORMAT:\n",
    "    {\n",
    "      \"prediction_class_id\": [Integer from Table],\n",
    "      \"prediction_label\": \"[Exact String from Table]\",\n",
    "      \"video_description\": \"[Concise description of the observed action]\",\n",
    "      \"hazard_detection\": {\n",
    "        \"is_hazardous\": [true/false based on the Hazard Status column],\n",
    "        \"temporal_segment\": \"[Start Time - End Time] or null\"\n",
    "      }\n",
    "    }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158be02f",
   "metadata": {},
   "source": [
    "### 6. Reload the dataset / Prepare inputs / Run Cosmos Reason 2 in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bd6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 40 videos...\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [47.6m elapsed, 0s remaining, 0.0 samples/s]    \n",
      "Processing complete. Launching App...\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the FiftyOne Dataset\n",
    "# Replace \"your_dataset_name\" with your actual dataset name\n",
    "dataset = fo.load_dataset(\"pjramg/Safe_Unsafe_Test\")\n",
    "\n",
    "# 2. Setup Model\n",
    "model, processor = load_model()\n",
    "transformers.set_seed(0)\n",
    "\n",
    "print(f\"Processing {len(dataset)} videos...\")\n",
    "\n",
    "# 3. Iterate through FiftyOne Samples\n",
    "# Use progress_bar to track status\n",
    "for sample in dataset.iter_samples(progress=True):\n",
    "    video_path = sample.filepath\n",
    "    \n",
    "    \n",
    "    # Prepare inputs\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_INSTRUCTIONS}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"video\", \"video\": video_path},\n",
    "                {\"type\": \"text\", \"text\": USER_PROMPT_CONTENT},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        inputs = processor.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            fps=4, # Set FPS here\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Inference\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :]\n",
    "            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )[0]\n",
    "\n",
    "        # 4. Parse and Save to FiftyOne\n",
    "        # Cosmos-Reason2 usually outputs clean JSON, but we wrap in try/except\n",
    "        try:\n",
    "            # Cleaning markdown blocks if model returns ```json ... ```\n",
    "            clean_json = output_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            json_data = json.loads(clean_json)\n",
    "            \n",
    "            # Store as a custom field \"cosmos_analysis\"\n",
    "            sample[\"cosmos_analysis\"] = json_data\n",
    "            \n",
    "            # Optional: Also add the label as a top-level classification for easy filtering\n",
    "            sample[\"safety_label\"] = fo.Classification(\n",
    "                label=json_data.get(\"prediction_label\"),\n",
    "                #confidence=1.0 # Model doesn't provide logprobs easily here\n",
    "            )\n",
    "            \n",
    "            sample.save()\n",
    "        except Exception as e:\n",
    "            print(f\"JSON Parsing failed for {video_path}: {e}\")\n",
    "            sample[\"cosmos_error\"] = str(output_text)\n",
    "            sample.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Inference failed for {video_path}: {e}\")\n",
    "\n",
    "print(\"Processing complete. Launching App...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d8ae2",
   "metadata": {},
   "source": [
    "### 7. Visualize the results in FiftyOne, compare results with Ground Truth and make adjustments if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc338d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=c896a0b2-da9e-4050-ba32-5b86e8c98673\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7574b08fa8a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook sessions cannot wait\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(dataset)\n",
    "session.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4428ee",
   "metadata": {},
   "source": [
    "### Output sample:\n",
    "\n",
    "<video src=\"../../../docs/recipes/inference/reason2/worker_safety/assets/output_sample.webm\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b75663",
   "metadata": {},
   "source": [
    "# Happy Coding!!! ü§ñ ü¶æ üß† üíª ‚å®Ô∏è üß© üõ†Ô∏è üß™ üîß"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
