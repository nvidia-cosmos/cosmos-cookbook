# Data curation pipeline for AV Reasoning

Sources:

* <https://gitlab-master.nvidia.com/dir/yotta/-/merge_requests/809#8d73ff9cb36712deca201f4f7ffa503b01f8f6a3>
* <https://gitlab-master.nvidia.com/dir/yotta/-/merge_requests/983>

Current scripts run locally, will be refactored to run on yotta.

## Captioning

Due to severe action hallucinations and critical context missing by Qwen2.5-VL (even after extensive prompt engineering), we decided to skip the model based context/action captioning and directly to extract and compose captions from human annotations.

For each video, we extract the core labeling information to compose as the caption.

```bash
python pipelines/reasoning/av_reasoning/captioning.py 
--dir_annotations /path/to/annotations
--dir_captions /path/to/captions 
--dir_clips /path/to/clips
```

Arguments:

- `dir_annotations`: Path to the folder containing raw human annotations
- `dir_captions`: Path to the folder where output will be saved
- `dir_clips`: Path to the folder containing video clips (front view)

We consider the following labeling information from human annotations.

- `description`: General description of ego driving behavior and related reasons, environment (e.g., scene type, time of day, weather condition, road condition), and critical objects (e.g., vehicles, pedestrians, cyclists, traffic lights, traffic signs) 
- `driving difficulity explanation`: A concise explanation about how difficult to drive in the scenario
- `notice`: Any events worth noting, with start and end time (e.g., ego driving behavior, signs and signals, road user communication, bad behavior) 

## Reasoning

For each video, we predict the next driving behavior and obtain the reasoning trace based on DS-R1.

```bash
python pipelines/reasoning/av_reasoning/reasoning.py 
--dir_captions /path/to/captions 
--dir_reasoning /path/to/reasoning
```

Arguments:

- `dir_captions`: Path to the folder containing captions generated by the captioning stage 
- `dir_clips`: Path to the folder where output will be saved