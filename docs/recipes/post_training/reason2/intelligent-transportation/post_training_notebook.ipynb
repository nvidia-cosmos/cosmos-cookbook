{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd81f351",
      "metadata": {},
      "source": [
        "#  Intelligent Transportation Post-Training with Cosmos Reason 2\n",
        "\n",
        "This notebook demonstrates how to fine-tune **NVIDIA Cosmos Reason 2** for intelligent transportation scene understanding using the **Woven Traffic Safety (WTS) Dataset**.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Supervised Fine-Tuning (SFT) aligns pre-trained models to specific tasks by showing clear input-output pairs. In this notebook, we fine-tune Cosmos Reason 2-8B to understand traffic scenes - including road attributes, pedestrian situations, and vehicle behavior.\n",
        "\n",
        "### What You'll Learn\n",
        "- Explore and visualize the WTS dataset\n",
        "- Configure training hyperparameters for optimal performance\n",
        "- Understand vision token calculations for different frame sampling strategies\n",
        "- Train and evaluate the model\n",
        "- Deploy with FP8 quantization and NVIDIA NIM\n",
        "\n",
        "### Table of Contents\n",
        "1. Environment Setup\n",
        "2. Dataset Exploration\n",
        "3. Zero-Shot Inference\n",
        "4. Training Configuration\n",
        "5. Run Training\n",
        "6. Run Evaluation\n",
        "7. Fine-Tuned Inference\n",
        "8. Deployment\n",
        "\n",
        "### Prerequisites\n",
        "- Downloaded WTS Dataset from [Woven by Toyota](https://woven-visionai.github.io/wts-dataset-homepage/)\n",
        "- NVIDIA GPUs (A100 recommended)\n",
        "- Python 3.10+ with pip or uv\n",
        "\n",
        "---\n",
        "\n",
        "**Reference:** [NVIDIA Cosmos Cookbook - Intelligent Transportation Post-Training](https://nvidia-cosmos.github.io/cosmos-cookbook/recipes/post_training/reason2/intelligent-transportation/post_training.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd2f6d1",
      "metadata": {},
      "source": [
        "## Environment Setup (Recommended: uv)\n",
        "\n",
        "Use uv to set up Cosmos Reason 2 and Cosmos-RL quickly. This step can take several minutes and requires sufficient disk space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee5fcb4-34b5-4f80-8c09-76b960826f43",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "PYTHON = sys.executable\n",
        "print(f\"Using Python: {PYTHON}\")\n",
        "\n",
        "# Repo paths (keep consistent with the Configuration section below)\n",
        "# Path to the cloned cosmos-reason2 repository\n",
        "COSMOS_REASON2_REPO = \"/home/ubuntu/cosmos-reason2\"\n",
        "\n",
        "# Path to the cloned cosmos-cookbook repository\n",
        "COSMOS_COOKBOOK_REPO = \"/home/ubuntu/cosmos-cookbook\"\n",
        "\n",
        "# Cosmos-RL directory (inside the cosmos-reason2 repo)\n",
        "COSMOS_RL_PATH = f\"{COSMOS_REASON2_REPO}/examples/cosmos_rl\"\n",
        "\n",
        "# Install ffmpeg and redis-server\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ffmpeg redis-server\n",
        "\n",
        "# Bootstrap pip\n",
        "print(\"üì¶ Bootstrapping pip...\")\n",
        "!$PYTHON -m ensurepip --upgrade 2>/dev/null || echo \"pip ready\"\n",
        "!$PYTHON -m pip install --upgrade pip\n",
        "\n",
        "# Install visualization dependencies\n",
        "print(\"\\nüì¶ Installing visualization dependencies...\")\n",
        "!$PYTHON -m pip install matplotlib numpy opencv-python pyyaml tqdm requests decord\n",
        "\n",
        "# Install uv\n",
        "print(\"üì¶ Installing uv...\")\n",
        "!{sys.executable} -m pip install -q uv\n",
        "\n",
        "# Clone repos if needed\n",
        "!git clone https://github.com/nvidia-cosmos/cosmos-reason2.git {COSMOS_REASON2_REPO} 2>/dev/null || echo \"cosmos-reason2 already exists\"\n",
        "!git clone https://github.com/nvidia-cosmos/cosmos-cookbook.git {COSMOS_COOKBOOK_REPO} 2>/dev/null || echo \"cosmos-cookbook already exists\"\n",
        "\n",
        "print(\"\\nüì¶ Setting up cosmos-reason2 prerequisites with uv sync...\")\n",
        "!cd {COSMOS_REASON2_REPO} && {sys.executable} -m uv sync --extra cu128\n",
        "\n",
        "print(\"\\nüì¶ Setting up cosmos-rl with uv sync...\")\n",
        "!cd {COSMOS_RL_PATH} && {sys.executable} -m uv sync\n",
        "\n",
        "print(\"\\n‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8ce70a",
      "metadata": {},
      "source": [
        "### Alternative: Manual pip Installation (Optional)\n",
        "\n",
        "If you prefer manual installs, use this cell instead of the uv-based setup above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c35f09-4f15-4dc7-ac09-33779e719b29",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Path to the cloned cosmos-reason2 repository\n",
        "COSMOS_REASON2_REPO = \"/home/ubuntu/cosmos-reason2\"\n",
        "\n",
        "# Path to the cloned cosmos-cookbook repository\n",
        "COSMOS_COOKBOOK_REPO = \"/home/ubuntu/cosmos-cookbook\"\n",
        "\n",
        "PYTHON = sys.executable\n",
        "print(f\"Using Python: {sys.executable}\\n\")\n",
        "\n",
        "# Bootstrap pip\n",
        "print(\"üì¶ Bootstrapping pip...\")\n",
        "!$PYTHON -m ensurepip --upgrade 2>/dev/null || echo \"pip ready\"\n",
        "!$PYTHON -m pip install --upgrade pip\n",
        "\n",
        "# Clone repos if needed\n",
        "print(\"\\nüì¶ Cloning repositories...\")\n",
        "!git clone https://github.com/nvidia-cosmos/cosmos-reason2.git {COSMOS_REASON2_REPO} 2>/dev/null || echo \"cosmos-reason2 already exists\"\n",
        "!git clone https://github.com/nvidia-cosmos/cosmos-cookbook.git {COSMOS_COOKBOOK_REPO} 2>/dev/null || echo \"cosmos-cookbook already exists\"\n",
        "\n",
        "# Install visualization dependencies\n",
        "print(\"\\nüì¶ Step 1: Installing visualization dependencies...\")\n",
        "!$PYTHON -m pip install matplotlib numpy opencv-python pyyaml tqdm requests decord\n",
        "\n",
        "# Install PyTorch with CUDA 12.8\n",
        "print(\"\\nüì¶ Step 2: Installing PyTorch with CUDA 12.8...\")\n",
        "!$PYTHON -m pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n",
        "\n",
        "# Verify CUDA\n",
        "print(\"\\nüîç Verifying CUDA...\")\n",
        "import torch\n",
        "print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')\n",
        "\n",
        "# Step 3: Install cosmos-rl\n",
        "print(\"\\nüì¶ Step 3: Installing cosmos-reason2-utils & cosmos-rl...\")\n",
        "!$PYTHON -m pip install /home/ubuntu/cosmos-reason2/cosmos_reason2_utils\n",
        "!$PYTHON -m pip install /home/ubuntu/cosmos-reason2/examples/cosmos_rl\n",
        "\n",
        "# Step 4: Install flash-attention (builds against torch 2.8.0)\n",
        "print(\"\\nüì¶ Step 4: Installing flash-attention...\")\n",
        "# os.environ[\"TMPDIR\"] = f\"{CACHE_DIR}/tmp\"\n",
        "os.environ[\"MAX_JOBS\"] = \"4\"\n",
        "!$PYTHON -m pip uninstall flash-attn -y 2>/dev/null || true\n",
        "!$PYTHON -m pip cache remove flash-attn 2>/dev/null || true\n",
        "!$PYTHON -m pip install flash-attn --no-build-isolation --no-cache-dir --force-reinstall\n",
        "!$PYTHON -m pip install einops\n",
        "\n",
        "# Verify\n",
        "try:\n",
        "    import flash_attn\n",
        "    print(f'‚úÖ flash-attn: {flash_attn.__version__}')\n",
        "except:\n",
        "    print(\"‚ùå flash-attn failed\")\n",
        "\n",
        "# Step 5: Install vllm\n",
        "print(\"\\nüì¶ Step 5: Installing vllm...\")\n",
        "!$PYTHON -m pip install vllm qwen-vl-utils\n",
        "\n",
        "# Install vllm for inference\n",
        "print(\"\\nüì¶ Step 6: Installing vllm...\")\n",
        "!$PYTHON -m pip install vllm qwen-vl-utils\n",
        "\n",
        "# Verify cache is being used\n",
        "print(f\"\\nüìÅ Cache usage:\")\n",
        "!du -sh {CACHE_DIR}/* 2>/dev/null || echo \"Cache empty (packages may already be installed)\"\n",
        "\n",
        "print(\"\\n‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa4f83c2",
      "metadata": {},
      "source": [
        "### Use the Cosmos Reason 2 Kernel (Required for Inference)\n",
        "\n",
        "After installing dependencies, switch the notebook to the cosmos-reason venv kernel:\n",
        "\n",
        "1. **Create a kernel for the venv (run once in a terminal):**\n",
        "   ```\n",
        "   cd /path/to/cosmos-reason2\n",
        "   source .venv/bin/activate\n",
        "   python -m pip install ipykernel\n",
        "   python -m ipykernel install --user --name cosmos-reason2-venv --display-name \"Cosmos-Reason\"\n",
        "   ```\n",
        "\n",
        "2. **Switch the notebook kernel:**\n",
        "- In JupyterLab: **Kernel ‚Üí Change Kernel‚Ä¶**\n",
        "- In Classic Notebook: **Kernel ‚Üí Change kernel**\n",
        "- Select **Cosmos-Reason2**\n",
        "\n",
        "4. **Verify in a cell:**\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0272995f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c99851d6",
      "metadata": {},
      "source": [
        "### Alternative: Docker Container (Optional)\n",
        "\n",
        "If you prefer running in a containerized environment, you can build and run the Cosmos Reason 2 Docker container. This requires Docker and the NVIDIA Container Toolkit.\n",
        "\n",
        "**Build the container:**\n",
        "The build command tags the image for reuse.\n",
        "\n",
        "**CUDA Variants:**\n",
        "- CUDA 12.8: `--build-arg=CUDA_VERSION=12.8.1` (default, requires NVIDIA Driver)\n",
        "- CUDA 13.0: `--build-arg=CUDA_VERSION=13.0.0` (required for DGX Spark and Jetson AGX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726e9e7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Docker Container Build (Optional)\n",
        "# Uncomment and run if using Docker instead of uv/pip\n",
        "\n",
        "# Build the container (run from cosmos-reason2 repo directory)\n",
        "# !cd {COSMOS_REASON2_REPO} && docker build -f Dockerfile --build-arg=CUDA_VERSION=12.8.1 -t cosmos-reason2:cu128 .\n",
        "\n",
        "# For CUDA 13.0 (DGX Spark / Jetson AGX):\n",
        "# !cd {COSMOS_REASON2_REPO} && docker build -f Dockerfile --build-arg=CUDA_VERSION=13.0.0 -t cosmos-reason2:cu130 .\n",
        "\n",
        "print(\"Docker build commands (uncomment to run):\")\n",
        "print(f\"  cd {COSMOS_REASON2_REPO}\")\n",
        "print(\"  docker build -f Dockerfile --build-arg=CUDA_VERSION=12.8.1 -t cosmos-reason2:cu128 .\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa5bedfd",
      "metadata": {},
      "source": [
        "**Run the container:**\n",
        "\n",
        "The container mounts the current directory to `/workspace` and preserves venv and cache directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c591143e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Docker Container Run (Optional)\n",
        "# Uncomment and customize before running\n",
        "\n",
        "# docker run -it --gpus all --ipc=host --rm \\\n",
        "#     -v .:/workspace \\\n",
        "#     -v /workspace/.venv \\\n",
        "#     -v /workspace/examples/cosmos_rl/.venv \\\n",
        "#     -v /root/.cache:/root/.cache \\\n",
        "#     -e HF_TOKEN=\"$HF_TOKEN\" \\\n",
        "#     cosmos-reason2:cu128\n",
        "\n",
        "print(\"Docker run command (uncomment to run):\")\n",
        "print(\"\"\"docker run -it --gpus all --ipc=host --rm \\\\\n",
        "    -v .:/workspace \\\\\n",
        "    -v /workspace/.venv \\\\\n",
        "    -v /workspace/examples/cosmos_rl/.venv \\\\\n",
        "    -v /root/.cache:/root/.cache \\\\\n",
        "    -e HF_TOKEN=\"$HF_TOKEN\" \\\\\n",
        "    cosmos-reason2:cu128\"\"\")\n",
        "\n",
        "print(\"\\nOptional arguments:\")\n",
        "print(\"  --ipc=host         Use host shared memory (torchrun needs this)\")\n",
        "print(\"  -v /root/.cache    Mount host cache to avoid re-downloads\")\n",
        "print(\"  -e HF_TOKEN        Pass HuggingFace token to container\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362352ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c739a0",
      "metadata": {},
      "source": [
        "### Verify Installation\n",
        "\n",
        "Confirm that core dependencies and the `cosmos-rl` CLI are available before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955b7cd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installations\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Verifying installations:\\n\")\n",
        "\n",
        "# Check visualization packages\n",
        "try:\n",
        "    import matplotlib\n",
        "    print(f\"  matplotlib: {matplotlib.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"  matplotlib: NOT installed ‚úó\")\n",
        "try:\n",
        "    import numpy\n",
        "    print(f\"  numpy: {numpy.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"  numpy: NOT installed ‚úó\")\n",
        "try:\n",
        "    import cv2\n",
        "    print(f\"  opencv: {cv2.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"  opencv: NOT installed ‚úó\")\n",
        "\n",
        "# Check cosmos-rl venv\n",
        "COSMOS_RL_PATH = \"/home/ubuntu/cosmos-reason2/examples/cosmos_rl\"\n",
        "COSMOS_RL_BIN = f\"{COSMOS_RL_PATH}/.venv/bin/cosmos-rl\"\n",
        "\n",
        "print(\"\\nüîç Checking cosmos-rl venv:\")\n",
        "!ls -la {COSMOS_RL_PATH}/.venv/bin/ 2>/dev/null | grep -E \"cosmos|python\" || echo \"venv not found\"\n",
        "\n",
        "if os.path.exists(COSMOS_RL_BIN):\n",
        "    print(f\"\\n‚úÖ cosmos-rl found!\")\n",
        "    print(\"\\nüìã cosmos-rl --help:\")\n",
        "    !{COSMOS_RL_BIN} --help 2>&1 | head -15\n",
        "else:\n",
        "    print(f\"\\n‚ùå cosmos-rl not found at {COSMOS_RL_BIN}\")\n",
        "    print(\"\\nüîß Try running uv sync manually:\")\n",
        "    !cd {COSMOS_RL_PATH} && {sys.executable} -m uv sync 2>&1 | tail -30"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "043c87a8",
      "metadata": {},
      "source": [
        "## Dataset Exploration\n",
        "\n",
        "Before post-training a vision-language model, it helps to inspect a few samples to understand clip length, camera viewpoints, and the kinds of questions and answers available. This quick check also confirms your dataset paths are correct and that annotations align with videos.\n",
        "\n",
        "For this notebook, we use the **Woven Traffic Safety (WTS) Dataset** (Environment VQA subset) as the example. It includes:\n",
        "- **255 traffic scenarios**\n",
        "- **1,200+ video segments**\n",
        "- **341 videos** with **~5.6k MCQ question-answer pairs**\n",
        "- Average video length is **~75 seconds**.\n",
        "\n",
        "Let's load and display a sample video from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb864fc",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set the dataset, model, and repo paths once here. The rest of the notebook references these variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf67581-9939-4013-8137-37a149d3bbc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Video, HTML, Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION - Update these paths before running the notebook\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Repository Paths ---\n",
        "# If you cloned the repos elsewhere, update these to match.\n",
        "# Path to the cloned cosmos-reason2 repository\n",
        "COSMOS_REASON2_REPO = \"/home/ubuntu/cosmos-reason2\"\n",
        "\n",
        "# Path to the cloned cosmos-cookbook repository (contains training scripts)\n",
        "COSMOS_COOKBOOK_REPO = \"/home/ubuntu/cosmos-cookbook\"\n",
        "\n",
        "# --- Dataset Paths ---\n",
        "# Training dataset directory (should contain videos/ and annotations.json)\n",
        "TRAIN_DATA_PATH = \"/ephemeral/wts_data_train\"\n",
        "\n",
        "# Validation dataset directory (should contain videos/ and annotations.json)\n",
        "VAL_DATA_PATH = \"/ephemeral/wts_data_val\"\n",
        "\n",
        "# --- Model Paths ---\n",
        "# Base model checkpoint (local path for Cosmos Reason 2 checkpoints or HuggingFace ID \"nvidia/Cosmos-Reason2-8B\")\n",
        "BASE_MODEL_PATH = \"/ephemeral/Cosmos-Reason2-8B\"\n",
        "\n",
        "# Output directory for fine-tuned model checkpoints\n",
        "FINETUNED_MODEL_PATH = \"/ephemeral/finetuned_model\"\n",
        "\n",
        "# Example video for quick testing (update to your video path)\n",
        "EXAMPLE_VIDEO_PATH = \"/home/ubuntu/example_video_wts.mp4\"\n",
        "\n",
        "# --- Derived Paths (computed from above, usually no need to edit) ---\n",
        "TRAIN_VIDEOS_PATH = f\"{TRAIN_DATA_PATH}/videos\"\n",
        "TRAIN_ANNOTATIONS_PATH = f\"{TRAIN_DATA_PATH}/annotations.json\"\n",
        "VAL_VIDEOS_PATH = f\"{VAL_DATA_PATH}/videos\"\n",
        "VAL_ANNOTATIONS_PATH = f\"{VAL_DATA_PATH}/annotations.json\"\n",
        "\n",
        "# Cosmos-RL directory (inside the cosmos-reason2 repo)\n",
        "COSMOS_RL_PATH = f\"{COSMOS_REASON2_REPO}/examples/cosmos_rl\"\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Train Dataset Path:   {TRAIN_DATA_PATH}\")\n",
        "print(f\"  Validation Dataset:   {VAL_DATA_PATH}\")\n",
        "print(f\"  Base Model Path:       {BASE_MODEL_PATH}\")\n",
        "print(f\"  Fine-Tuned Model Path: {FINETUNED_MODEL_PATH}\")\n",
        "print(f\"  Example Video Path:    {EXAMPLE_VIDEO_PATH}\")\n",
        "print(f\"  Cosmos Reason2 Repo:   {COSMOS_REASON2_REPO}\")\n",
        "print(f\"  Cosmos Cookbook Repo:  {COSMOS_COOKBOOK_REPO}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52cd11c",
      "metadata": {},
      "source": [
        "### Authenticate to Hugging Face (Optional)\n",
        "\n",
        "If you are downloading the Cosmos Reason 2 model from Hugging Face (e.g., `nvidia/Cosmos-Reason2-8B`), you need to authenticate. This cell prompts for your HF token and performs authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb14fc35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace Authentication (Optional)\n",
        "import subprocess\n",
        "import getpass\n",
        "from IPython.display import display, HTML\n",
        "import time\n",
        "\n",
        "display(HTML('<a href=\"https://huggingface.co/settings/tokens\" target=\"_blank\" style=\"font-size:16px;\">üîë Get HuggingFace Token</a>'))\n",
        "time.sleep(2)\n",
        "\n",
        "hf_token = getpass.getpass(\"HuggingFace Token (leave blank to skip): \").strip()\n",
        "\n",
        "if hf_token:\n",
        "    result = subprocess.run(\n",
        "        [\"uvx\", \"hf\", \"auth\", \"login\", \"--token\", hf_token],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    print(\"‚úÖ HuggingFace login successful\" if result.returncode == 0 else f\"‚ùå Failed: {result.stderr}\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipped HuggingFace authentication\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d2057c5",
      "metadata": {},
      "source": [
        "### Video Helper Utilities\n",
        "\n",
        "These helpers list videos, display metadata, and sample frames so you can quickly validate the dataset contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3fdf173",
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_videos(video_dir, num_samples=5):\n",
        "    \"\"\"List available videos in the dataset directory.\"\"\"\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
        "    videos = []\n",
        "    \n",
        "    video_path = Path(video_dir)\n",
        "    if video_path.exists():\n",
        "        for ext in video_extensions:\n",
        "            videos.extend(list(video_path.rglob(f\"*{ext}\")))\n",
        "    \n",
        "    return videos[:num_samples] if videos else []\n",
        "\n",
        "def display_video_with_info(video_path, width=640):\n",
        "    \"\"\"Display a video with metadata information.\"\"\"\n",
        "    import cv2\n",
        "    \n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    width_px = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height_px = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duration = frame_count / fps if fps > 0 else 0\n",
        "    cap.release()\n",
        "    \n",
        "    print(f\"üìπ Video: {video_path.name}\")\n",
        "    print(f\"   Resolution: {width_px} x {height_px}\")\n",
        "    print(f\"   FPS: {fps:.2f}\")\n",
        "    print(f\"   Duration: {duration:.2f} seconds\")\n",
        "    print(f\"   Total Frames: {frame_count}\")\n",
        "    \n",
        "    return Video(str(video_path), embed=True, width=width)\n",
        "\n",
        "def extract_sample_frames(video_path, num_frames=8):\n",
        "    \"\"\"Extract uniformly sampled frames from a video (mimics nframes=8 config).\"\"\"\n",
        "    import cv2\n",
        "    \n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    # Uniformly sample frame indices\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    \n",
        "    frames = []\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame_rgb)\n",
        "    cap.release()\n",
        "    \n",
        "    return frames, indices\n",
        "\n",
        "# List and display sample videos\n",
        "print(\"üîç Searching for videos in WTS dataset...\\n\")\n",
        "\n",
        "train_videos_path = os.path.join(TRAIN_VIDEOS_PATH)\n",
        "sample_videos = list_videos(train_videos_path)\n",
        "\n",
        "if sample_videos:\n",
        "    print(f\"Found {len(sample_videos)} sample videos:\\n\")\n",
        "    for i, v in enumerate(sample_videos):\n",
        "        print(f\"  {i+1}. {v.name}\")\n",
        "    \n",
        "    # Display the first video\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Displaying first video:\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    \n",
        "    display(display_video_with_info(sample_videos[0]))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No videos found. Please update TRAIN_VIDEOS_PATH to your dataset location.\")\n",
        "    print(f\"   Current path: {train_videos_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "903e532c",
      "metadata": {},
      "source": [
        "## Dataset Labels and Annotations\n",
        "\n",
        "The WTS dataset provides rich annotations including:\n",
        "- **Textual descriptions** of pedestrian and vehicle behavior\n",
        "- **Traffic VQA** with multiple-choice questions (MCQ)\n",
        "\n",
        "The data is converted to **Llava dataset format** for training - a JSON structure with conversation pairs between human queries and expected VLM responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "294712c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_llava_format(example):\n",
        "    \"\"\"Pretty print a Llava-format example from the dataset.\"\"\"\n",
        "    print(\"üìã Llava Dataset Format Example (from WTS):\")\n",
        "    print(\"=\"*60)\n",
        "    print(json.dumps(example, indent=2))\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "def parse_mcq_text(text):\n",
        "    \"\"\"Parse MCQ question/options from the WTS Llava-format prompt.\"\"\"\n",
        "    cleaned = text.replace(\"<video>\", \" \").strip()\n",
        "    lines = [line.strip() for line in cleaned.splitlines() if line.strip()]\n",
        "    question = lines[0] if lines else \"\"\n",
        "    options = lines[1:] if len(lines) > 1 else []\n",
        "    return question, options\n",
        "\n",
        "\n",
        "def is_correct_option(option, answer):\n",
        "    \"\"\"Mark the correct option based on the answer token (e.g., 'A').\"\"\"\n",
        "    opt = option.strip()\n",
        "    ans = answer.strip()\n",
        "    if not ans:\n",
        "        return False\n",
        "    prefixes = [f\"{ans}:\", f\"{ans})\", f\"{ans}.\", f\"{ans} \"]\n",
        "    return opt == ans or any(opt.startswith(prefix) for prefix in prefixes)\n",
        "\n",
        "\n",
        "# Load actual MCQ examples from the training annotations\n",
        "annotations_path = TRAIN_ANNOTATIONS_PATH\n",
        "\n",
        "if not os.path.exists(annotations_path):\n",
        "    print(\"‚ö†Ô∏è annotations.json not found. Update TRAIN_DATA_PATH to your dataset location.\")\n",
        "    print(f\"   Current path: {annotations_path}\")\n",
        "else:\n",
        "    with open(annotations_path, \"r\") as f:\n",
        "        annotations = json.load(f)\n",
        "\n",
        "    # Display a real Llava-format entry\n",
        "    if annotations:\n",
        "        display_llava_format(annotations[0])\n",
        "\n",
        "    # Display a few actual MCQ questions\n",
        "    print(\"\\n\\nüìù Sample MCQ Questions from the Training Set:\")\n",
        "    print(\"=\"*60)\n",
        "    for i, ann in enumerate(annotations[:4], 1):\n",
        "        question_text, options = parse_mcq_text(ann[\"conversations\"][0][\"value\"])\n",
        "        answer = ann[\"conversations\"][1][\"value\"]\n",
        "        print(f\"\\nQ{i}: {question_text}\")\n",
        "        for opt in options:\n",
        "            marker = \"‚úì\" if is_correct_option(opt, answer) else \" \"\n",
        "            print(f\"   [{marker}] {opt}\")\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288d3d31",
      "metadata": {},
      "source": [
        "## Inference Helper Class\n",
        "\n",
        "Define a reusable inference helper so we can run zero-shot evaluation before training and reuse the same logic after fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d360a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference Class for Cosmos Reason 2\n",
        "class CosmosReason2Inference:\n",
        "    \"\"\"\n",
        "    Inference wrapper for fine-tuned Cosmos Reason 2 model.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_path, nframes=8, max_tokens=512):\n",
        "        \"\"\"\n",
        "        Initialize the inference engine.\n",
        "        \n",
        "        Args:\n",
        "            model_path: Path to the model checkpoint (base or fine-tuned)\n",
        "            nframes: Number of frames to sample from videos\n",
        "            max_tokens: Maximum tokens to generate\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.nframes = nframes\n",
        "        self.max_tokens = max_tokens\n",
        "        self.llm = None\n",
        "        self.processor = None\n",
        "        self.sampling_params = None\n",
        "        \n",
        "    def load_model(self):\n",
        "        \"\"\"Load the model using vLLM.\"\"\"\n",
        "        try:\n",
        "            from vllm import LLM, SamplingParams\n",
        "            from transformers import AutoProcessor\n",
        "            import torch\n",
        "            import gc\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "            print(f\"üîÑ Loading model from: {self.model_path}\")\n",
        "            \n",
        "            self.llm = LLM(\n",
        "                model=self.model_path,\n",
        "                tensor_parallel_size=1,\n",
        "                max_model_len=32768,\n",
        "                trust_remote_code=True,\n",
        "                limit_mm_per_prompt={\"video\": 1, \"image\": 0}\n",
        "            )\n",
        "            \n",
        "            # Load processor for chat template\n",
        "            self.processor = AutoProcessor.from_pretrained(\n",
        "                self.model_path,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            self.sampling_params = SamplingParams(\n",
        "                max_tokens=self.max_tokens,\n",
        "                temperature=0.0\n",
        "            )\n",
        "            \n",
        "            print(\"‚úÖ Model loaded successfully!\")\n",
        "            return True\n",
        "            \n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è vLLM not installed. Install with: pip install vllm\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading model: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def query(self, video_path, question, system_prompt=\"You are a helpful assistant.\"):\n",
        "        \"\"\"\n",
        "        Query the model with a video and question.\n",
        "        \n",
        "        Args:\n",
        "            video_path: Path to the video file\n",
        "            question: Question to ask about the video\n",
        "            system_prompt: System prompt for the model\n",
        "        \n",
        "        Returns:\n",
        "            Model's response as string\n",
        "        \"\"\"\n",
        "        if self.llm is None or not hasattr(self, 'processor') or self.processor is None:\n",
        "            print(\"‚ö†Ô∏è Model not loaded. Call load_model() first.\")\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            from qwen_vl_utils import process_vision_info\n",
        "            \n",
        "            # Prepare messages with video\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"video\", \"video\": str(video_path), \"nframes\": self.nframes},\n",
        "                    {\"type\": \"text\", \"text\": question}\n",
        "                ]}\n",
        "            ]\n",
        "            \n",
        "            # Apply chat template to get text prompt\n",
        "            text_prompt = self.processor.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "            \n",
        "            # Extract video data using process_vision_info\n",
        "            image_inputs, video_inputs, video_kwargs = process_vision_info(\n",
        "                messages,\n",
        "                image_patch_size=16,\n",
        "                return_video_kwargs=True,\n",
        "                return_video_metadata=True\n",
        "            )\n",
        "            \n",
        "            # Prepare input for vLLM generate\n",
        "            model_input = {\n",
        "                \"prompt\": text_prompt,\n",
        "                \"multi_modal_data\": {\"video\": video_inputs},\n",
        "                \"mm_processor_kwargs\": video_kwargs\n",
        "            }\n",
        "            \n",
        "            # Run inference using generate (not chat)\n",
        "            outputs = self.llm.generate([model_input], self.sampling_params)\n",
        "            response = outputs[0].outputs[0].text\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except ImportError as ie:\n",
        "            print(f\"‚ö†Ô∏è Import error: {ie}\")\n",
        "            print(\"   Install with: pip install qwen-vl-utils\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during inference: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "    \n",
        "    def batch_query(self, queries):\n",
        "        \"\"\"\n",
        "        Process multiple queries in batch.\n",
        "        \n",
        "        Args:\n",
        "            queries: List of (video_path, question) tuples\n",
        "        \n",
        "        Returns:\n",
        "            List of responses\n",
        "        \"\"\"\n",
        "        responses = []\n",
        "        for video_path, question in queries:\n",
        "            response = self.query(video_path, question)\n",
        "            responses.append(response)\n",
        "        return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44523cc3",
      "metadata": {},
      "source": [
        "## Zero-Shot Inference\n",
        "\n",
        "Before fine-tuning, run a quick zero-shot evaluation with the base model to establish a baseline for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c185561e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot inference with base model\n",
        "print(\"=\"*70)\n",
        "print(\"üîç ZERO-SHOT INFERENCE (Base Model)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "inference_base = CosmosReason2Inference(\n",
        "    model_path=BASE_MODEL_PATH,  # Base model path\n",
        "    nframes=8,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "inference_base.load_model()\n",
        "\n",
        "# Test video\n",
        "zero_shot_video = EXAMPLE_VIDEO_PATH\n",
        "print(f\"\\nVideo: {zero_shot_video}\\n\")\n",
        "\n",
        "# Sample question\n",
        "question = \"What is the pedestrian doing in this video?\"\n",
        "print(\"üìù Question:\")\n",
        "print(question)\n",
        "print(\"-\"*70)\n",
        "\n",
        "response = inference_base.query(zero_shot_video, question)\n",
        "print(f\"\\n‚úÖ ANSWER: {response}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Clean up GPU memory before training\n",
        "try:\n",
        "    del inference_base\n",
        "    import torch, gc\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf72f6c",
      "metadata": {},
      "source": [
        "## Training Configuration\n",
        "\n",
        "The training configuration is specified in a TOML file. Key hyperparameters are optimized for training on **8x A100 GPUs**. Adjust the other parameters according to the hardware.\n",
        "\n",
        "### Key Configuration Highlights:\n",
        "- **Learning Rate**: 2e-5 with cosine decay\n",
        "- **Batch Size**: 32 per replica\n",
        "- **Model**: nvidia/Cosmos-Reason2-2B (or 8B)\n",
        "- **Max Length**: 32,768 tokens\n",
        "- **Vision**: 8 frames uniformly sampled (nframes=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abfa83de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the official training config from the cosmos-cookbook repo\n",
        "CONFIG_FILE = \"scripts/examples/reason2/intelligent-transportation/sft_config.toml\"\n",
        "\n",
        "CONFIG_PATH = f\"{COSMOS_COOKBOOK_REPO}/{CONFIG_FILE}\"\n",
        "\n",
        "# Display the raw config file\n",
        "print(\"üìÑ Official Training Config from cosmos-cookbook\")\n",
        "print(f\"   Source: github.com/nvidia-cosmos/cosmos-cookbook/{CONFIG_FILE}\\n\")\n",
        "print(\"=\"*70)\n",
        "!cat {CONFIG_PATH}\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Parse and show key parameters\n",
        "try:\n",
        "    import tomllib\n",
        "except ImportError:\n",
        "    try:\n",
        "        import tomli as tomllib\n",
        "    except:\n",
        "        import pip._vendor.tomli as tomllib\n",
        "\n",
        "with open(CONFIG_PATH, \"rb\") as f:\n",
        "    config = tomllib.load(f)\n",
        "\n",
        "print(\"\\nüîë Key Training Parameters:\\n\")\n",
        "print(f\"  Model:           {config['policy']['model_name_or_path']}\")\n",
        "print(f\"  Learning Rate:   {config['train']['optm_lr']}\")\n",
        "print(f\"  Batch Size:      {config['train']['train_batch_per_replica']} per GPU\")\n",
        "print(f\"  Max Seq Length:  {config['policy']['model_max_length']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b767b97a",
      "metadata": {},
      "source": [
        "## Update Training Config Paths\n",
        "\n",
        "Patch the `sft_config.toml` file with your local dataset and output paths. This keeps the training script aligned with your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba629ed3-7359-454b-83fc-508797456c35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update sft_config.toml with actual paths\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "CONFIG_PATH = f\"{COSMOS_COOKBOOK_REPO}/scripts/examples/reason2/intelligent-transportation/sft_config.toml\"\n",
        "print(\"üìù Updating sft_config.toml with actual paths...\\n\")\n",
        "\n",
        "# Use sed to update the config file directly (in-place)\n",
        "subprocess.run([\"sed\", \"-i\", f's|annotation_path = .*|annotation_path = \"{TRAIN_ANNOTATIONS_PATH}\"|', CONFIG_PATH])\n",
        "subprocess.run([\"sed\", \"-i\", f's|media_path = .*|media_path = \"{TRAIN_VIDEOS_PATH}\"|', CONFIG_PATH])\n",
        "subprocess.run([\"sed\", \"-i\", f's|output_dir = .*|output_dir = \"{FINETUNED_MODEL_PATH}\"|', CONFIG_PATH])\n",
        "\n",
        "print(f\"  annotation_path: {TRAIN_ANNOTATIONS_PATH}\")\n",
        "print(f\"  media_path:      {TRAIN_VIDEOS_PATH}\")\n",
        "print(f\"  output_dir:      {FINETUNED_MODEL_PATH}\")\n",
        "\n",
        "# Verify paths exist\n",
        "print(\"\\nüîç Verifying paths:\")\n",
        "ann_path = TRAIN_ANNOTATIONS_PATH\n",
        "media_path = TRAIN_VIDEOS_PATH\n",
        "\n",
        "if os.path.exists(ann_path):\n",
        "    print(f\"  ‚úÖ annotations.json exists\")\n",
        "else:\n",
        "    print(f\"  ‚ùå annotations.json NOT found at {ann_path}\")\n",
        "    \n",
        "if os.path.exists(media_path):\n",
        "    print(f\"  ‚úÖ videos directory exists\")\n",
        "    video_files = list(Path(media_path).rglob(\"*.mp4\"))\n",
        "    print(f\"     Found {len(video_files)} video files\")\n",
        "else:\n",
        "    print(f\"  ‚ùå videos directory NOT found at {media_path}\")\n",
        "\n",
        "# Show updated config section\n",
        "print(\"\\nüìÑ Updated [custom.dataset] section:\")\n",
        "print(\"=\"*50)\n",
        "!grep -A5 \"\\[custom.dataset\\]\" {CONFIG_PATH}\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4fc999",
      "metadata": {},
      "source": [
        "## Vision Token Calculation (Ablation Study)\n",
        "\n",
        "Understanding how vision tokens are calculated is crucial for optimizing training. Qwen3-VL (the backbone of Cosmos Reason 2) compresses input videos in both **space** and **time**:\n",
        "\n",
        "### Compression Factors:\n",
        "- **Spatial Compression**: Effective patch size = 32 (16 patch √ó 2 spatial merge)\n",
        "- **Temporal Compression**: Effective temporal step = 2 (2 frames merge into 1)\n",
        "\n",
        "### Two Ablation Configurations:\n",
        "1. **nframes=8 (3k tokens)**: Fewer frames, higher resolution per frame\n",
        "2. **fps=1, 8M pixels (8k tokens)**: More frames, lower resolution per frame\n",
        "\n",
        "**Key Finding**: Higher resolution per frame (3k tokens) achieves better accuracy with 3√ó faster training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e05322e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vision Token Calculation for Cosmos Reason 2\n",
        "\n",
        "class VisionTokenCalculator:\n",
        "    \"\"\"Calculator for vision tokens in Qwen3-VL based models.\"\"\"\n",
        "    \n",
        "    # Model constants\n",
        "    PATCH_SIZE = 16\n",
        "    SPATIAL_MERGE = 2\n",
        "    TEMPORAL_MERGE = 2  # 2 frames merge into 1\n",
        "    \n",
        "    EFFECTIVE_PATCH_SIZE = PATCH_SIZE * SPATIAL_MERGE  # 32\n",
        "    DEFAULT_MAX_FRAME_TOKENS = 768  # Default max tokens per frame\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.patch_area = self.EFFECTIVE_PATCH_SIZE ** 2  # 32 * 32 = 1024\n",
        "    \n",
        "    def calculate_tokens_nframes(self, nframes, frame_width, frame_height, max_frame_tokens=768):\n",
        "        \"\"\"\n",
        "        Calculate vision tokens for nframes configuration.\n",
        "        \n",
        "        Args:\n",
        "            nframes: Number of frames to sample uniformly\n",
        "            frame_width: Original frame width\n",
        "            frame_height: Original frame height\n",
        "            max_frame_tokens: Maximum tokens per frame (default 768)\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with calculation details\n",
        "        \"\"\"\n",
        "        # Calculate max pixels per frame from max tokens\n",
        "        max_pixels_per_frame = max_frame_tokens * self.patch_area\n",
        "        \n",
        "        # Check if resizing is needed\n",
        "        original_pixels = frame_width * frame_height\n",
        "        \n",
        "        if original_pixels > max_pixels_per_frame:\n",
        "            # Need to resize - maintain aspect ratio\n",
        "            scale = (max_pixels_per_frame / original_pixels) ** 0.5\n",
        "            new_width = int(frame_width * scale)\n",
        "            new_height = int(frame_height * scale)\n",
        "            # Round to nearest multiple of patch size\n",
        "            new_width = (new_width // self.EFFECTIVE_PATCH_SIZE) * self.EFFECTIVE_PATCH_SIZE\n",
        "            new_height = (new_height // self.EFFECTIVE_PATCH_SIZE) * self.EFFECTIVE_PATCH_SIZE\n",
        "        else:\n",
        "            new_width, new_height = frame_width, frame_height\n",
        "        \n",
        "        # Calculate tokens per frame\n",
        "        tokens_per_frame = (new_width * new_height) // self.patch_area\n",
        "        \n",
        "        # Apply temporal compression\n",
        "        effective_frames = nframes // self.TEMPORAL_MERGE\n",
        "        \n",
        "        # Total vision tokens\n",
        "        total_tokens = effective_frames * tokens_per_frame\n",
        "        \n",
        "        return {\n",
        "            \"config\": f\"nframes={nframes}\",\n",
        "            \"original_resolution\": f\"{frame_width} √ó {frame_height}\",\n",
        "            \"resized_resolution\": f\"{new_width} √ó {new_height}\",\n",
        "            \"frames_sampled\": nframes,\n",
        "            \"effective_frames\": effective_frames,\n",
        "            \"tokens_per_frame\": tokens_per_frame,\n",
        "            \"total_vision_tokens\": total_tokens\n",
        "        }\n",
        "    \n",
        "    def calculate_tokens_fps(self, video_duration_sec, fps, total_pixel_limit):\n",
        "        \"\"\"\n",
        "        Calculate vision tokens for fps configuration with total pixel limit.\n",
        "        \n",
        "        Args:\n",
        "            video_duration_sec: Video duration in seconds\n",
        "            fps: Frames per second to sample\n",
        "            total_pixel_limit: Maximum total pixels across all frames\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with calculation details\n",
        "        \"\"\"\n",
        "        # Calculate number of frames\n",
        "        num_frames = int(video_duration_sec * fps)\n",
        "        # Round to nearest even number for temporal compression\n",
        "        num_frames = (num_frames // 2) * 2\n",
        "        \n",
        "        # Calculate effective frames after temporal merge\n",
        "        effective_frames = num_frames // self.TEMPORAL_MERGE\n",
        "        \n",
        "        # Calculate pixels per frame based on total limit\n",
        "        pixels_per_frame = total_pixel_limit // effective_frames\n",
        "        \n",
        "        # Calculate frame dimensions (approximate, assuming ~16:9 aspect ratio)\n",
        "        aspect_ratio = 16 / 9\n",
        "        frame_height = int((pixels_per_frame / aspect_ratio) ** 0.5)\n",
        "        frame_width = int(frame_height * aspect_ratio)\n",
        "        \n",
        "        # Round to patch size multiples\n",
        "        frame_width = (frame_width // self.EFFECTIVE_PATCH_SIZE) * self.EFFECTIVE_PATCH_SIZE\n",
        "        frame_height = (frame_height // self.EFFECTIVE_PATCH_SIZE) * self.EFFECTIVE_PATCH_SIZE\n",
        "        \n",
        "        # Recalculate actual pixels\n",
        "        actual_pixels_per_frame = frame_width * frame_height\n",
        "        \n",
        "        # Tokens per frame\n",
        "        tokens_per_frame = actual_pixels_per_frame // self.patch_area\n",
        "        \n",
        "        # Total vision tokens\n",
        "        total_tokens = effective_frames * tokens_per_frame\n",
        "        \n",
        "        return {\n",
        "            \"config\": f\"fps={fps}, total_pixels={total_pixel_limit:,}\",\n",
        "            \"video_duration\": f\"{video_duration_sec} seconds\",\n",
        "            \"frames_sampled\": num_frames,\n",
        "            \"effective_frames\": effective_frames,\n",
        "            \"pixels_per_frame\": actual_pixels_per_frame,\n",
        "            \"resized_resolution\": f\"{frame_width} √ó {frame_height}\",\n",
        "            \"tokens_per_frame\": tokens_per_frame,\n",
        "            \"total_vision_tokens\": total_tokens\n",
        "        }\n",
        "\n",
        "# Initialize calculator\n",
        "calc = VisionTokenCalculator()\n",
        "\n",
        "print(\"üî¢ Vision Token Calculations for Ablation Study\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüìê Model Constants:\")\n",
        "print(f\"   Patch Size: {calc.PATCH_SIZE}\")\n",
        "print(f\"   Spatial Merge: {calc.SPATIAL_MERGE}x\")\n",
        "print(f\"   Effective Patch Size: {calc.EFFECTIVE_PATCH_SIZE}\")\n",
        "print(f\"   Temporal Merge: {calc.TEMPORAL_MERGE}x (2 frames ‚Üí 1)\")\n",
        "print(f\"   Patch Area: {calc.patch_area} pixels\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a32767",
      "metadata": {},
      "source": [
        "## Run Training\n",
        "\n",
        "Now we launch the SFT training using the Cosmos-RL framework. The training uses:\n",
        "- **8√ó A100 GPUs (Recommended, can be done with 4x A100 too)** with data parallelism\n",
        "- **Supervised Fine-Tuning (SFT)** on MCQ data\n",
        "\n",
        "Training time: ~1 hour 16 minutes for 3k vision tokens configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d2f02c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Training with Cosmos-RL (using cosmos-rl's own venv)\n",
        "import os\n",
        "import sys\n",
        "\n",
        "COSMOS_RL_VENV = f\"{COSMOS_RL_PATH}/.venv\"\n",
        "TRAINING_DIR = f\"{COSMOS_COOKBOOK_REPO}/scripts/examples/reason2/intelligent-transportation\"\n",
        "\n",
        "print(\"üöÄ Running Training with Cosmos-RL\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Working Dir: {TRAINING_DIR}\")\n",
        "print(f\"  Config:      sft_config.toml\")\n",
        "print(f\"  Script:      custom_sft.py\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check Redis package installed\n",
        "try:\n",
        "    import redis\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\"Redis not installed. Install with: pip install redis\") from exc\n",
        "\n",
        "print(\"\\n‚è±Ô∏è Expected training time (8√ó A100):\")\n",
        "print(\"   - 3k tokens (nframes=8): ~1h 16m for 3 epochs\")\n",
        "\n",
        "# Setup cosmos-rl venv if needed\n",
        "if not os.path.exists(f\"{COSMOS_RL_VENV}/bin/cosmos-rl\"):\n",
        "    print(\"\\nüì¶ Setting up cosmos-rl venv with uv sync...\")\n",
        "    !cd {COSMOS_RL_PATH} && pip install -q uv && uv sync\n",
        "\n",
        "# Run training - MUST activate venv so subprocesses get the right python\n",
        "print(\"\\nüîÑ Starting training...\\n\")\n",
        "!source {COSMOS_RL_VENV}/bin/activate && cd {TRAINING_DIR} && cosmos-rl --config sft_config.toml custom_sft.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03e4af2",
      "metadata": {},
      "source": [
        "## Run Evaluation\n",
        "\n",
        "After training, we evaluate the model on the validation set of the WTS Environment VQA dataset:\n",
        "- **171 videos** with **2.6k MCQ questions** (unseen during training)\n",
        "- Evaluation uses **vLLM** inference engine for efficient batch processing\n",
        "- Metrics: **Accuracy** on multiple-choice questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0b8a5cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Evaluation using cosmos-cookbook script\n",
        "import sys\n",
        "import os\n",
        "\n",
        "EVAL_DIR = f\"{COSMOS_COOKBOOK_REPO}/scripts/examples/reason2/intelligent-transportation\"\n",
        "EVAL_CONFIG = f\"{EVAL_DIR}/eval_config.yaml\"\n",
        "\n",
        "# Update paths in eval_config.yaml (preserve rest of config)\n",
        "print(\"\\nüìù Updating paths in eval_config.yaml...\")\n",
        "\n",
        "# Use subprocess for proper variable expansion (in-place edit)\n",
        "import subprocess\n",
        "subprocess.run([\"sed\", \"-i\", f's|annotation_path:.*|annotation_path: {VAL_ANNOTATIONS_PATH}|', EVAL_CONFIG])\n",
        "subprocess.run([\"sed\", \"-i\", f's|media_dir:.*|media_dir: {VAL_VIDEOS_PATH}|', EVAL_CONFIG])\n",
        "subprocess.run([\"sed\", \"-i\", f's|model_name:.*|model_name: {FINETUNED_MODEL_PATH}|', EVAL_CONFIG])\n",
        "\n",
        "print(f\"  annotation_path: {VAL_ANNOTATIONS_PATH}\")\n",
        "print(f\"  media_dir:       {VAL_VIDEOS_PATH}\")\n",
        "print(f\"  model_name:      {FINETUNED_MODEL_PATH}\")\n",
        "\n",
        "# Show updated config\n",
        "print(\"\\nüìÑ Evaluation Config:\")\n",
        "print(\"=\"*70)\n",
        "!cat {EVAL_CONFIG}\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Run evaluation\n",
        "print(\"\\nüîÑ Starting evaluation...\\n\")\n",
        "!cd {EVAL_DIR} && {sys.executable} evaluate.py --config eval_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfbc5f6",
      "metadata": {},
      "source": [
        "## Results Visualization\n",
        "\n",
        "Visualize training results and accuracy comparisons across different configurations and training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb87d73f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display results from evaluate.py\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "\n",
        "EVAL_DIR = f\"{COSMOS_COOKBOOK_REPO}/scripts/examples/reason2/intelligent-transportation\"\n",
        "RESULTS_BASE = os.path.join(EVAL_DIR, \"results\")\n",
        "\n",
        "# Find all results.json files from evaluate.py output\n",
        "result_files = glob.glob(os.path.join(RESULTS_BASE, \"**/results.json\"), recursive=True)\n",
        "\n",
        "if result_files:\n",
        "    # Use the most recent results file\n",
        "    result_file = max(result_files, key=os.path.getmtime)\n",
        "    \n",
        "    with open(result_file, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"üìä EVALUATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\n   Accuracy:  {metrics['accuracy']*100:.2f}%\")\n",
        "    print(f\"   Correct:   {metrics['total_correct']} / {metrics['total_questions']}\")\n",
        "    print(f\"\\n   Results:   {result_file}\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "else:\n",
        "    print(f\"‚ÑπÔ∏è No results found in {RESULTS_BASE}\")\n",
        "    print(\"   Run the evaluation cell above first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba3add6",
      "metadata": {},
      "source": [
        "## Inference with Fine-Tuned Model\n",
        "\n",
        "Run inference on custom traffic videos using the fine-tuned Cosmos Reason 2 model. The model can answer both MCQ and open-ended questions about traffic scenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b982f20e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tuned inference demo\n",
        "inference = CosmosReason2Inference(model_path=FINETUNED_MODEL_PATH, nframes=8)\n",
        "inference.load_model()\n",
        "\n",
        "# Sample questions for traffic scene understanding\n",
        "SAMPLE_QUESTIONS = [\n",
        "    \"What type of road is shown in this video?\",\n",
        "    \"How many vehicles can you see in the scene?\",\n",
        "    \"Is there any pedestrian in the video? If yes, what are they doing?\",\n",
        "    \"What potential traffic hazards do you observe?\",\n",
        "    \"Describe the overall traffic flow and density.\",\n",
        "]\n",
        "\n",
        "# Test video path (update with your video)\n",
        "test_video = EXAMPLE_VIDEO_PATH\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üé¨ TESTING INFERENCE ON TRAFFIC VIDEO (FINE-TUNED)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Video: {test_video}\")\n",
        "print(f\"Total Questions: {len(SAMPLE_QUESTIONS)}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "for i, question in enumerate(SAMPLE_QUESTIONS, 1):\n",
        "    print(f\"üìù Question {i}/{len(SAMPLE_QUESTIONS)}\")\n",
        "    print(\"-\"*70)\n",
        "    print(question)\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    response = inference.query(test_video, question)\n",
        "    \n",
        "    print(f\"‚úÖ ANSWER: {response}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ All questions processed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08b6c821",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7a4057",
      "metadata": {},
      "source": [
        "## Deployment with FP8 Quantization and NVIDIA NIM\n",
        "\n",
        "For production deployment, we can:\n",
        "1. **Quantize to FP8** for faster inference with minimal accuracy loss\n",
        "2. **Deploy on NVIDIA NIM** for optimized, production-ready inference microservices\n",
        "\n",
        "### Benefits of FP8 Quantization:\n",
        "- 2√ó faster inference\n",
        "- 50% memory reduction\n",
        "- Minimal accuracy degradation (<0.5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b25981d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# FP8 Quantization Configuration\n",
        "# Output directory for the FP8-quantized model\n",
        "FP8_MODEL_OUTPUT_PATH = f\"{FINETUNED_MODEL_PATH}_fp8\"\n",
        "\n",
        "QUANTIZATION_CONFIG = {\n",
        "    \"model_path\": FINETUNED_MODEL_PATH,\n",
        "    \"output_path\": FP8_MODEL_OUTPUT_PATH,\n",
        "    \"precision\": \"fp8\"\n",
        "}\n",
        "\n",
        "print(\"üîß FP8 Quantization Setup\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Quantization script path\n",
        "quantize_script = f\"{COSMOS_REASON2_REPO}/scripts/quantize.py\"\n",
        "\n",
        "# Quantization command\n",
        "QUANTIZE_CMD = f\"\"\"\n",
        "# FP8 Quantization Command\n",
        "# ========================\n",
        "\n",
        "python {quantize_script} \\\\\n",
        "    --model \"{QUANTIZATION_CONFIG['model_path']}\" \\\\\n",
        "    -o \"{QUANTIZATION_CONFIG['output_path']}\" \\\\\n",
        "    --precision {QUANTIZATION_CONFIG['precision']}\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù Quantization Command:\")\n",
        "print(\"-\"*70)\n",
        "print(QUANTIZE_CMD)\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Expected benefits\n",
        "print(\"\\nüìä Expected Benefits of FP8 Quantization:\")\n",
        "print(\"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Metric                 ‚îÇ FP16         ‚îÇ FP8          ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Model Size             ‚îÇ ~16 GB       ‚îÇ ~8 GB        ‚îÇ\n",
        "‚îÇ Inference Speed        ‚îÇ 1.0√ó         ‚îÇ ~2.0√ó        ‚îÇ\n",
        "‚îÇ GPU Memory Usage       ‚îÇ 100%         ‚îÇ ~50%         ‚îÇ\n",
        "‚îÇ Accuracy Loss          ‚îÇ 0%           ‚îÇ <0.5%        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e3dfad",
      "metadata": {},
      "source": [
        "## Run FP8 Quantization\n",
        "\n",
        "Quantize the fine-tuned checkpoint to FP8 to reduce memory usage and speed up inference. Ensure the Cosmos-RL venv is available before running this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32284f86",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "COSMOS_RL_VENV = f\"{COSMOS_RL_PATH}/.venv\"\n",
        "\n",
        "# Run FP8 Quantization (Shell Command)\n",
        "!source {COSMOS_RL_VENV}/bin/activate && {COSMOS_REASON2_REPO}/scripts/quantize.py \\\n",
        "    --model \"{QUANTIZATION_CONFIG['model_path']}\" \\\n",
        "    -o \"{QUANTIZATION_CONFIG['output_path']}\" \\\n",
        "    --precision fp8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc8df964",
      "metadata": {},
      "source": [
        "## Authenticate to NGC\n",
        "\n",
        "You need an NGC API key to pull the Cosmos Reason 2 NIM image. This cell prompts for your key and performs a Docker login."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6a3784",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NGC Login\n",
        "import subprocess\n",
        "import getpass\n",
        "from IPython.display import display, HTML\n",
        "import time\n",
        "\n",
        "display(HTML('<a href=\"https://org.ngc.nvidia.com/setup/api-key\" target=\"_blank\" style=\"font-size:16px;\">üîë Get NGC API Key</a>'))\n",
        "time.sleep(2)\n",
        "\n",
        "ngc_api_key = getpass.getpass(\"NGC API Key: \").strip()\n",
        "\n",
        "if ngc_api_key:\n",
        "    result = subprocess.run(\n",
        "        [\"docker\", \"login\", \"nvcr.io\", \"-u\", \"$oauthtoken\", \"--password-stdin\"],\n",
        "        input=ngc_api_key, text=True, capture_output=True\n",
        "    )\n",
        "    print(\"‚úÖ Login successful\" if result.returncode == 0 else f\"‚ùå Failed: {result.stderr}\")\n",
        "else:\n",
        "    print(\"‚ùå No key provided\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcded35",
      "metadata": {},
      "source": [
        "## NIM Deployment Configuration\n",
        "\n",
        "Define the model path, NIM image, and runtime parameters before launching the container. Adjust `max_model_len` and `allowed_local_media_path` based on your hardware and data location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71fc9b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NVIDIA NIM Deployment Configuration\n",
        "NIM_CONFIG = {\n",
        "    \"model_path\": f\"{QUANTIZATION_CONFIG['output_path']}/model_fp8\",  # FP8 quantized model\n",
        "    \"nim_image\": \"nvcr.io/nim/nvidia/cosmos-reason2-8b:latest\",\n",
        "    \"model_name\": \"cosmos-reason2-wts-fp8\",\n",
        "    \"port\": 8000,\n",
        "    \"shm_size\": \"32GB\",\n",
        "    \"max_model_len\": 262144,  # 256k tokens, reduce for lower memory usage\n",
        "    \"allowed_local_media_path\": \"/home/ubuntu\"  # Allow local video file access\n",
        "}\n",
        "\n",
        "print(\"üöÄ NVIDIA NIM Deployment\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {NIM_CONFIG['model_path']}\")\n",
        "print(f\"Max Context Length: {NIM_CONFIG['max_model_len']:,} tokens\")\n",
        "print(f\"Port: {NIM_CONFIG['port']}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d2536d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shell Command for NIM Deployment\n",
        "\n",
        "NIM_DEPLOY_CMD = f\"\"\"\n",
        "# Set environment variables\n",
        "export CUSTOM_WEIGHTS=\"{NIM_CONFIG['model_path']}\"\n",
        "export NIM_IMAGE=\"{NIM_CONFIG['nim_image']}\"\n",
        "\n",
        "# Launch NIM container\n",
        "docker run -d --name=cosmos-reason2-wts \\\\\n",
        "    --gpus all \\\\\n",
        "    --shm-size={NIM_CONFIG['shm_size']} \\\\\n",
        "    -e NIM_MODEL_NAME=$CUSTOM_WEIGHTS \\\\\n",
        "    -e NIM_SERVED_MODEL_NAME=\"{NIM_CONFIG['model_name']}\" \\\\\n",
        "    -e NIM_MAX_MODEL_LEN={NIM_CONFIG['max_model_len']} \\\\\n",
        "    -e NIM_ALLOWED_LOCAL_MEDIA_PATH=\"{NIM_CONFIG['allowed_local_media_path']}\" \\\\\n",
        "    -v $CUSTOM_WEIGHTS:$CUSTOM_WEIGHTS \\\\\n",
        "    -v {NIM_CONFIG['allowed_local_media_path']}:{NIM_CONFIG['allowed_local_media_path']}:ro \\\\\n",
        "    -u $(id -u) \\\\\n",
        "    -p {NIM_CONFIG['port']}:8000 \\\\\n",
        "    $NIM_IMAGE\n",
        "\n",
        "# Wait for startup (takes ~2-3 minutes)\n",
        "# Check the deployment status using \n",
        "docker logs -f cosmos-reason2-wts\n",
        "\n",
        "# Health check\n",
        "curl http://localhost:{NIM_CONFIG['port']}/v1/health/ready | jq .\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"\\nüí° Steps:\")\n",
        "print(\"   1. Run the commands below\")\n",
        "print(\"   2. Monitor with: docker logs -f cosmos-reason2-wts\")\n",
        "print(\"   3. Stop with: docker stop cosmos-reason2-wts \\n\")\n",
        "print(\"-\"*70)\n",
        "print(\"üìù NIM Deployment Commands:\")\n",
        "print(\"-\"*70)\n",
        "print(NIM_DEPLOY_CMD)\n",
        "print(\"-\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f1206b",
      "metadata": {},
      "source": [
        "## Test NIM API\n",
        "\n",
        "This step sends a sample request to the local NIM endpoint to confirm the deployment is responding correctly. It uses a remote video URL for convenience, but you can swap in your own video later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee95337",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## Test NIM API - Remote Video\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "NIM_ENDPOINT = f\"http://localhost:{NIM_CONFIG['port']}/v1/chat/completions\"\n",
        "\n",
        "# Example 1: Remote video URL\n",
        "test_payload_remote = {\n",
        "    \"model\": NIM_CONFIG['model_name'],\n",
        "    \"messages\": [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"What is in this video?\"},\n",
        "            {\"type\": \"video_url\", \"video_url\": {\"url\": \"https://download.samplelib.com/mp4/sample-5s.mp4\"}}\n",
        "        ]\n",
        "    }],\n",
        "    \"media_io_kwargs\": {\n",
        "        \"video\": {\n",
        "            \"num_frames\": 10\n",
        "        }\n",
        "    },\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "print(\"Testing with remote video URL...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    response = requests.post(NIM_ENDPOINT, json=test_payload_remote)\n",
        "    result = response.json()\n",
        "    \n",
        "    if \"choices\" in result:\n",
        "        print(\"Success!\")\n",
        "        print(f\"\\nQuestion: What is in this video?\")\n",
        "        print(f\"Answer: {result['choices'][0]['message']['content']}\")\n",
        "        print(f\"\\nTokens used: {result['usage']['total_tokens']:,}\")\n",
        "    else:\n",
        "        print(\"Error:\", result)\n",
        "except Exception as e:\n",
        "    print(f\"Connection error: {e}\")\n",
        "    print(\"   Make sure the NIM container is running!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e0811b",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "- **CUDA out of memory**: reduce batch size, lower `nframes`, or decrease `model_max_length`; restart the kernel to clear GPU memory.\n",
        "- **Flash Attention build failures**: confirm CUDA/Torch versions match, ensure build tools are installed\n",
        "- **Redis not found**: install with `pip install redis` before running training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdeb17fb",
      "metadata": {},
      "source": [
        "## Summary & Conclusion\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "1. **Data Exploration**: Visualized WTS traffic videos and annotations\n",
        "2. **Training Configuration**: Set up optimal hyperparameters for SFT\n",
        "3. **Vision Token Analysis**: Understood the tradeoffs between frame count and resolution\n",
        "4. **Model Training**: Fine-tuned Cosmos Reason 2 on traffic VQA data\n",
        "5. **Evaluation**: Achieved 93.65% accuracy on validation set\n",
        "6. **Deployment**: Prepared FP8 quantization and NIM deployment\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "| Insight | Implication |\n",
        "|---------|-------------|\n",
        "| Higher resolution > More frames | Prioritize image quality over quantity for scene understanding |\n",
        "| Fast convergence | Domain-specific data enables quick training (~1 hour) |\n",
        "| MCQ ‚Üí Open-ended | Fine-tuning on MCQs improves open-ended reasoning |\n",
        "| FP8 quantization | 2√ó speedup with minimal accuracy loss |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- [ ] Experiment with other traffic datasets\n",
        "- [ ] Try different frame sampling strategies\n",
        "- [ ] Evaluate on edge cases (night, rain, occlusions)\n",
        "- [ ] Deploy to production with monitoring\n",
        "\n",
        "---\n",
        "\n",
        "**Reference:** [NVIDIA Cosmos Cookbook](https://nvidia-cosmos.github.io/cosmos-cookbook/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
