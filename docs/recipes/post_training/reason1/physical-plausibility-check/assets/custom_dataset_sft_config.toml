# Training Configuration for Custom Dataset (Transfer1)
# Dataset: Custom labeled videos with physical plausibility scores
# Model: Cosmos-Reason1-7B
# 
# This configuration is optimized for 8 GPUs. Adjust dp_shard_size based on your setup:
# - 2 GPUs: dp_shard_size = 2
# - 4 GPUs: dp_shard_size = 4
# - 8 GPUs: dp_shard_size = 8

[custom.dataset]
# Path to training dataset with conversation format
path = "data/transfer1_split_with_conv/train"

[train]
# Training epochs and output configuration
epoch = 10
output_dir = "outputs/transfer1_sft"
compile = false
train_batch_per_replica = 32

# Evaluation configuration
eval_steps = 50
evaluation_strategy = "steps"
save_strategy = "steps"
load_best_model_at_end = true
metric_for_best_model = "eval_loss"

[policy]
# Model configuration
model_name_or_path = "nvidia/Cosmos-Reason1-7B"
model_max_length = 4096

[logging]
# Logging configuration
logger = ['console', 'tensorboard']
project_name = "cosmos_reason1"
experiment_name = "post_training_hf/transfer1_sft"

[train.train_policy]
# Training policy configuration
type = "sft"
conversation_column_name = "conversations"
mini_batch = 4

[train.eval_policy]
# Evaluation dataset configuration
dataset.name = "data/transfer1_split_with_conv/eval"

[train.ckpt]
# Checkpoint configuration
enable_checkpoint = true
save_freq = 50
max_keep = 5
save_mode = "async"

[policy.parallelism]
# Parallelism configuration
tp_size = 1
cp_size = 1
dp_shard_size = 8
pp_size = 1
dp_replicate_size = 1


